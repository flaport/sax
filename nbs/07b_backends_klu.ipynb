{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1db95e7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp backends.klu"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27c3489e",
   "metadata": {},
   "source": [
    "# Backend - KLU\n",
    "\n",
    "> SAX KLU Backend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19542d05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hide\n",
    "import sax\n",
    "import matplotlib.pyplot as plt\n",
    "from fastcore.test import test_eq\n",
    "from pytest import approx, raises\n",
    "from nbdev import show_doc\n",
    "\n",
    "import os, sys; sys.stderr = open(os.devnull, \"w\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12cf5743",
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "from __future__ import annotations\n",
    "\n",
    "from typing import Dict\n",
    "\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from sax.typing_ import SDense, SDict, SType, scoo\n",
    "\n",
    "try:\n",
    "    import klujax\n",
    "except ImportError:\n",
    "    klujax = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64e42767",
   "metadata": {},
   "source": [
    "## Citation\n",
    "The KLU backend is using `klujax`, which uses the [SuiteSparse](https://github.com/DrTimothyAldenDavis/SuiteSparse) C++ libraries for sparse matrix evaluations to evaluate the circuit insanely fast on a CPU. The specific algorith being used in question is the KLU algorithm:\n",
    "\n",
    "> Ekanathan Palamadai Natariajan. \"*KLU - A high performance sparse linear solver for circuit simulation problems.*\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2db5821c",
   "metadata": {},
   "source": [
    "## Theoretical Background"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d4d4a4c",
   "metadata": {},
   "source": [
    "The core of the KLU algorithm is supported by `klujax`, which internally uses the Suitesparse libraries to solve the sparse system `Ax = b`, in which A is a sparse matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1985f58",
   "metadata": {},
   "source": [
    "Now it only comes down to shoehorn our circuit evaluation into a sparse linear system of equations $Ax=b$ where we need to solve for $x$ using `klujax`. \n",
    "Consider the block diagonal matrix $S_{bd}$ of all components in the circuit acting on the fields $x_{in}$ at each of the individual ports of each of the component integrated in $S^{bd}$. The output fields $x^{out}$ at each of those ports is then given by:\n",
    "\n",
    "$$\n",
    "x^{out} = S_{bd} x^{in}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c26c04a",
   "metadata": {},
   "source": [
    "However, $S_{bd}$ is not the S-matrix of the circuit as it does not encode any connectivity *between* the components. Connecting two component ports basically comes down to enforcing equality between the output fields at one port of a component with the input fields at another port of another (or maybe even the same) component. This equality can be enforced by creating an internal connection matrix, connecting all internal ports of the circuit:\n",
    "\n",
    "$$\n",
    "x^{in} = C_{int} x^{out}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16b3fdad",
   "metadata": {},
   "source": [
    "We can thus write the following combined equation:\n",
    "\n",
    "$$\n",
    "x^{in} = C_{int} S_{bd} x^{in}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5ceb9a2",
   "metadata": {},
   "source": [
    "But this is not the complete story... Some component ports will *not* be *interconnected* with other ports: they will become the new *external ports* (or output ports) of the combined circuit. We can include those external ports into the above equation as follows:\n",
    "\n",
    "$$\n",
    "\\begin{pmatrix} x^{in} \\\\ x^{out}_{ext} \\end{pmatrix} = \\begin{pmatrix} C_{int} & C_{ext} \\\\ C_{ext}^T & 0 \\end{pmatrix} \\begin{pmatrix} S_{bd} x^{in} \\\\ x_{ext}^{in} \\end{pmatrix} \n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6382a242",
   "metadata": {},
   "source": [
    "Note that $C_{ext}$ is obviously **not** a square matrix. Eliminating $x^{in}$ from the equation above finally yields:\n",
    "\n",
    "$$\n",
    "x^{out}_{ext} = C^T_{ext} S_{bd} (\\mathbb{1} - C_{int}S_{bd})^{-1} C_{ext}x_{ext}^{in}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f15bdcd",
   "metadata": {},
   "source": [
    "We basically found a representation of the circuit S-matrix:\n",
    "\n",
    "$$\n",
    "S = C^T_{ext} S_{bd} (\\mathbb{1} - C_{int}S_{bd})^{-1} C_{ext}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40ad83b8",
   "metadata": {},
   "source": [
    "Obviously, we won't want to calculate the inverse $(\\mathbb{1} - C_{int}S_{bd})^{-1}$, which is the inverse of a very sparse matrix (a connection matrix only has a single 1 per line), which very often is not even sparse itself. In stead we'll use the `solve_klu` function:\n",
    "\n",
    "$$\n",
    "S = C^T_{ext} S_{bd} \\texttt{solve}\\_\\texttt{klu}\\left((\\mathbb{1} - C_{int}S_{bd}), C_{ext}\\right)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "018fab3d",
   "metadata": {},
   "source": [
    "Moreover, $C_{ext}^TS_{bd}$ is also a sparse matrix, therefore we'll also need a `mul_coo` routine:\n",
    "\n",
    "$$\n",
    "S = C^T_{ext} \\texttt{mul}\\_\\texttt{coo}\\left(S_{bd},~~\\texttt{solve}\\_\\texttt{klu}\\left((\\mathbb{1} - C_{int}S_{bd}),~C_{ext}\\right)\\right)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9b78852",
   "metadata": {},
   "source": [
    "## Sparse Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "521638fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hide_input\n",
    "show_doc(klujax.solve, doc_string=False, name=\"klujax.solve\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01299d5d",
   "metadata": {},
   "source": [
    "`klujax.solve` solves the sparse system of equations `Ax=b` for `x`. Where `A` is represented by in [COO-format](https://en.wikipedia.org/wiki/Sparse_matrix#Coordinate_list_(COO)) as (`Ai`, `Aj`, `Ax`).\n",
    "\n",
    "> Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63e7977a",
   "metadata": {},
   "outputs": [],
   "source": [
    "Ai = jnp.array([0, 1, 2, 3, 4])\n",
    "Aj = jnp.array([1, 3, 4, 0, 2])\n",
    "Ax = jnp.array([5, 6, 1, 1, 2])\n",
    "b =  jnp.array([5, 3, 2, 6, 1])\n",
    "\n",
    "x = klujax.solve(Ai, Aj, Ax, b)\n",
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3233b04c",
   "metadata": {},
   "source": [
    "This result is indeed correct:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "629bbc71",
   "metadata": {},
   "outputs": [],
   "source": [
    "A = jnp.zeros((5, 5)).at[Ai, Aj].set(Ax)\n",
    "print(A)\n",
    "print(A@x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7fe6547",
   "metadata": {},
   "source": [
    "However, to use this function effectively, we probably need an extra dimension for `Ax`. Indeed, we would like to solve this equation for multiple wavelengths (or more general, for multiple circuit configurations) at once. For this we can use `jax.vmap` to expose `klujax.solve` to more dimensions for `Ax`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f9686a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# exports\n",
    "solve_klu = None\n",
    "if klujax is not None:\n",
    "    solve_klu = jax.vmap(klujax.solve, (None, None, 0, None), 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0fcc3e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hide_input\n",
    "show_doc(solve_klu, doc_string=False, name=\"solve_klu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6707049d",
   "metadata": {},
   "source": [
    "Let's now redefine `Ax` and see what it gives:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4d7b6f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "Ai = jnp.array([0, 1, 2, 3, 4])\n",
    "Aj = jnp.array([1, 3, 4, 0, 2])\n",
    "Ax = jnp.array([[5, 6, 1, 1, 2], \n",
    "                [5, 4, 3, 2, 1], \n",
    "                [1, 2, 3, 4, 5]])\n",
    "b =  jnp.array([5, 3, 2, 6, 1])\n",
    "\n",
    "x = solve_klu(Ai, Aj, Ax, b)\n",
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f342c6f",
   "metadata": {},
   "source": [
    "This result is indeed correct:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0afefaa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "A = jnp.zeros((3, 5, 5)).at[:, Ai, Aj].set(Ax)\n",
    "jnp.einsum(\"ijk,ik->ij\", A, x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ed8f62d",
   "metadata": {},
   "source": [
    "Additionally, we need a way to multiply a sparse COO-matrix with a dense vector. This can be done with `klujax.coo_mul_vec`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "606b3e20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hide_input\n",
    "\n",
    "show_doc(klujax.coo_mul_vec, doc_string=False, name=\"klujax.coo_mul_vec\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f57c443",
   "metadata": {},
   "source": [
    "However, it's useful to allow a batch dimension, this time *both* in `Ax` and in `b`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f11c4df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# exporti\n",
    "\n",
    "# @jax.jit  # TODO: make this available to autograd\n",
    "# def mul_coo(Ai, Aj, Ax, b):\n",
    "#     result = jnp.zeros_like(b).at[..., Ai, :].add(Ax[..., :, None] * b[..., Aj, :])\n",
    "#     return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2f56966",
   "metadata": {},
   "outputs": [],
   "source": [
    "# exports\n",
    "mul_coo = None \n",
    "if klujax is not None:\n",
    "    mul_coo = jax.vmap(klujax.coo_mul_vec, (None, None, 0, 0), 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3e5e645",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hide_input\n",
    "show_doc(mul_coo, doc_string=False, name=\"mul_coo\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d26cc3d9",
   "metadata": {},
   "source": [
    "Let's confirm this does the right thing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd868973",
   "metadata": {},
   "outputs": [],
   "source": [
    "mul_coo(Ai, Aj, Ax, x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d71e89fa",
   "metadata": {},
   "source": [
    "## Circuit Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e77e2026",
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def evaluate_circuit_klu(\n",
    "    instances: Dict[str, SType],\n",
    "    connections: Dict[str, str],\n",
    "    ports: Dict[str, str],\n",
    "):\n",
    "    \"\"\"evaluate a circuit using KLU for the given sdicts. \"\"\"\n",
    "\n",
    "    if klujax is None:\n",
    "        raise ImportError(\n",
    "            \"Could not import 'klujax'. \"\n",
    "            \"Please install it first before using backend method 'klu'\"\n",
    "        )\n",
    "\n",
    "    assert solve_klu is not None\n",
    "    assert mul_coo is not None\n",
    "\n",
    "    connections = {**connections, **{v: k for k, v in connections.items()}}\n",
    "    inverse_ports = {v: k for k, v in ports.items()}\n",
    "    port_map = {k: i for i, k in enumerate(ports)}\n",
    "\n",
    "    idx, Si, Sj, Sx, instance_ports = 0, [], [], [], {}\n",
    "    batch_shape = ()\n",
    "    for name, instance in instances.items():\n",
    "        si, sj, sx, ports_map = scoo(instance)\n",
    "        Si.append(si + idx)\n",
    "        Sj.append(sj + idx)\n",
    "        Sx.append(sx)\n",
    "        if len(sx.shape[:-1]) > len(batch_shape):\n",
    "            batch_shape = sx.shape[:-1]\n",
    "        instance_ports.update({f\"{name},{p}\": i + idx for p, i in ports_map.items()})\n",
    "        idx += len(ports_map)\n",
    "\n",
    "    Si = jnp.concatenate(Si, -1)\n",
    "    Sj = jnp.concatenate(Sj, -1)\n",
    "    Sx = jnp.concatenate(\n",
    "        [jnp.broadcast_to(sx, (*batch_shape, sx.shape[-1])) for sx in Sx], -1\n",
    "    )\n",
    "\n",
    "    n_col = idx\n",
    "    n_rhs = len(port_map)\n",
    "\n",
    "    Cmap = {\n",
    "        int(instance_ports[k]): int(instance_ports[v]) for k, v in connections.items()\n",
    "    }\n",
    "    Ci = jnp.array(list(Cmap.keys()), dtype=jnp.int32)\n",
    "    Cj = jnp.array(list(Cmap.values()), dtype=jnp.int32)\n",
    "\n",
    "    Cextmap = {int(instance_ports[k]): int(port_map[v]) for k, v in inverse_ports.items()}\n",
    "    Cexti = jnp.stack(list(Cextmap.keys()), 0)\n",
    "    Cextj = jnp.stack(list(Cextmap.values()), 0)\n",
    "    Cext = jnp.zeros((n_col, n_rhs), dtype=complex).at[Cexti, Cextj].set(1.0)\n",
    "\n",
    "    # TODO: make this block jittable...\n",
    "    Ix = jnp.ones((*batch_shape, n_col))\n",
    "    Ii = Ij = jnp.arange(n_col)\n",
    "    mask = Cj[None,:] == Si[:, None]\n",
    "    CSi = jnp.broadcast_to(Ci[None, :], mask.shape)[mask]\n",
    "\n",
    "    # CSi = jnp.where(Cj[None, :] == Si[:, None], Ci[None, :], 0).sum(1)\n",
    "    mask = (Cj[:, None] == Si[None, :]).any(0)\n",
    "    CSj = Sj[mask]\n",
    "    \n",
    "    if Sx.ndim > 1: # bug in JAX... see https://github.com/google/jax/issues/9050\n",
    "        CSx = Sx[..., mask]\n",
    "    else:\n",
    "        CSx = Sx[mask]\n",
    "        \n",
    "    # CSj = jnp.where(mask, Sj, 0)\n",
    "    # CSx = jnp.where(mask, Sx, 0.0)\n",
    "\n",
    "    I_CSi = jnp.concatenate([CSi, Ii], -1)\n",
    "    I_CSj = jnp.concatenate([CSj, Ij], -1)\n",
    "    I_CSx = jnp.concatenate([-CSx, Ix], -1)\n",
    "\n",
    "    n_col, n_rhs = Cext.shape\n",
    "    n_lhs = jnp.prod(jnp.array(batch_shape, dtype=jnp.int32))\n",
    "    Sx = Sx.reshape(n_lhs, -1)\n",
    "    I_CSx = I_CSx.reshape(n_lhs, -1)\n",
    "\n",
    "    inv_I_CS_Cext = solve_klu(I_CSi, I_CSj, I_CSx, Cext)\n",
    "    S_inv_I_CS_Cext = mul_coo(Si, Sj, Sx, inv_I_CS_Cext)\n",
    "\n",
    "    CextT_S_inv_I_CS_Cext = S_inv_I_CS_Cext[..., Cexti, :][..., :, Cextj]\n",
    "    \n",
    "    _, n, _ = CextT_S_inv_I_CS_Cext.shape\n",
    "    S = CextT_S_inv_I_CS_Cext.reshape(*batch_shape, n, n)\n",
    "\n",
    "    return S, port_map"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dce9d8ca",
   "metadata": {},
   "source": [
    "## Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f577522a",
   "metadata": {},
   "outputs": [],
   "source": [
    "wg_sdict: SDict = {\n",
    "    (\"in0\", \"out0\"): 0.5 + 0.86603j,\n",
    "    (\"out0\", \"in0\"): 0.5 + 0.86603j,\n",
    "}\n",
    "\n",
    "τ, κ = 0.5 ** 0.5, 1j * 0.5 ** 0.5\n",
    "dc_sdense: SDense = (\n",
    "    jnp.array([[0, 0, τ, κ], \n",
    "               [0, 0, κ, τ], \n",
    "               [τ, κ, 0, 0], \n",
    "               [κ, τ, 0, 0]]),\n",
    "    {\"in0\": 0, \"in1\": 1, \"out0\": 2, \"out1\": 3},\n",
    ")\n",
    "\n",
    "mzi_sdense: SDense = evaluate_circuit_klu(\n",
    "    instances={\n",
    "        \"dc1\": dc_sdense,\n",
    "        \"wg\": wg_sdict,\n",
    "        \"dc2\": dc_sdense,\n",
    "    },\n",
    "    connections={\n",
    "        \"dc1,out0\": \"wg,in0\",\n",
    "        \"wg,out0\": \"dc2,in0\",\n",
    "        \"dc1,out1\": \"dc2,in1\",\n",
    "    },\n",
    "    ports={\n",
    "        \"in0\": \"dc1,in0\",\n",
    "        \"in1\": \"dc1,in1\",\n",
    "        \"out0\": \"dc2,out0\",\n",
    "        \"out1\": \"dc2,out1\",\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60b6d1e6",
   "metadata": {},
   "source": [
    "the KLU backend yields `SDense` results by default:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecc3115e",
   "metadata": {},
   "outputs": [],
   "source": [
    "mzi_sdense"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b8e87f3",
   "metadata": {},
   "source": [
    "An `SDense` is returned for perfomance reasons. By returning an `SDense` by default we prevent any internal `SDict -> SDense` conversions in deeply hierarchical circuits. It's however very easy to convert `SDense` to `SDict` as a final step. To do this, wrap the result (or the function generating the result) with `sdict`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7202ab1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "sax.sdict(mzi_sdense)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f8cd5ea",
   "metadata": {},
   "source": [
    "## Algorithm Walkthrough"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "665c856d",
   "metadata": {},
   "outputs": [],
   "source": [
    "instances={\n",
    "    \"dc1\": dc_sdense,\n",
    "    \"wg\": wg_sdict,\n",
    "    \"dc2\": dc_sdense,\n",
    "}\n",
    "connections={\n",
    "    \"dc1,out0\": \"wg,in0\",\n",
    "    \"wg,out0\": \"dc2,in0\",\n",
    "    \"dc1,out1\": \"dc2,in1\",\n",
    "}\n",
    "ports={\n",
    "    \"in0\": \"dc1,in0\",\n",
    "    \"in1\": \"dc1,in1\",\n",
    "    \"out0\": \"dc2,out0\",\n",
    "    \"out1\": \"dc2,out1\",\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5bf5188",
   "metadata": {},
   "source": [
    "Let's first enforce $C^T = C$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d7d4c39",
   "metadata": {},
   "outputs": [],
   "source": [
    "connections = {**connections, **{v: k for k, v in connections.items()}}\n",
    "connections"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a41250ec",
   "metadata": {},
   "source": [
    "We'll also need the reversed ports:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a70463f",
   "metadata": {},
   "outputs": [],
   "source": [
    "inverse_ports = {v: k for k, v in ports.items()}\n",
    "inverse_ports"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e96443b",
   "metadata": {},
   "source": [
    "An the port indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e18d182",
   "metadata": {},
   "outputs": [],
   "source": [
    "port_map = {k: i for i, k in enumerate(ports)}\n",
    "port_map"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70924074",
   "metadata": {},
   "source": [
    "Let's now create the COO-representation of our block diagonal S-matrix $S_{bd}$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "835deb8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx, Si, Sj, Sx, instance_ports = 0, [], [], [], {}\n",
    "batch_shape = ()\n",
    "for name, instance in instances.items():\n",
    "    si, sj, sx, ports_map = scoo(instance)\n",
    "    Si.append(si + idx)\n",
    "    Sj.append(sj + idx)\n",
    "    Sx.append(sx)\n",
    "    if len(sx.shape[:-1]) > len(batch_shape):\n",
    "        batch_shape = sx.shape[:-1]\n",
    "    instance_ports.update({f\"{name},{p}\": i + idx for p, i in ports_map.items()})\n",
    "    idx += len(ports_map)\n",
    "Si = jnp.concatenate(Si, -1)\n",
    "Sj = jnp.concatenate(Sj, -1)\n",
    "Sx = jnp.concatenate([jnp.broadcast_to(sx, (*batch_shape, sx.shape[-1])) for sx in Sx], -1)\n",
    "\n",
    "print(Si)\n",
    "print(Sj)\n",
    "print(Sx)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8edefa1",
   "metadata": {},
   "source": [
    "note that we also kept track of the `batch_shape`, i.e. the number of independent simulations (usually number of wavelengths). In the example being used here we don't have a batch dimension (all elements of the `SDict` are `0D`):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7513759a",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c63b14d7",
   "metadata": {},
   "source": [
    "We'll also keep track of the number of columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89000432",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_col = idx\n",
    "n_col"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b0f95da",
   "metadata": {},
   "source": [
    "And we'll need to solve the circuit for each output port, i.e. we need to solve `n_rhs` number of equations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27902354",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_rhs = len(port_map)\n",
    "n_rhs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05056b46",
   "metadata": {},
   "source": [
    "We can represent the internal connection matrix $C_{int}$ as a mapping between port indices:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3ff1fa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "Cmap = {int(instance_ports[k]): int(instance_ports[v]) for k, v in connections.items()}\n",
    "Cmap"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "999164af",
   "metadata": {},
   "source": [
    "Therefore, the COO-representation of this connection matrix can be obtained as follows (note that an array of values Cx is not necessary, all non-zero elements in a connection matrix are 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e4f1728",
   "metadata": {},
   "outputs": [],
   "source": [
    "Ci = jnp.array(list(Cmap.keys()), dtype=jnp.int32)\n",
    "Cj = jnp.array(list(Cmap.values()), dtype=jnp.int32)\n",
    "print(Ci)\n",
    "print(Cj)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47d5ed60",
   "metadata": {},
   "source": [
    "We can represent the external connection matrix $C_{ext}$ as a map between internal port indices and external port indices:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f56ae166",
   "metadata": {},
   "outputs": [],
   "source": [
    "Cextmap = {int(instance_ports[k]): int(port_map[v]) for k, v in inverse_ports.items()}\n",
    "Cextmap"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2485c01d",
   "metadata": {},
   "source": [
    "Just as for the internal matrix we can represent this external connection matrix in COO-format:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfb419ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "Cexti = jnp.stack(list(Cextmap.keys()), 0)\n",
    "Cextj = jnp.stack(list(Cextmap.values()), 0)\n",
    "print(Cexti)\n",
    "print(Cextj)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e46d609c",
   "metadata": {},
   "source": [
    "However, we actually need it as a dense representation:\n",
    "\n",
    "> help needed: can we find a way later on to keep this sparse?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25c4ec6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "Cext = jnp.zeros((n_col, n_rhs), dtype=complex).at[Cexti, Cextj].set(1.0)\n",
    "Cext"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b31cc279",
   "metadata": {},
   "source": [
    "We'll now calculate the row index `CSi` of $C_{int}S_{bd}$ in COO-format:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a7e87d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: make this block jittable...\n",
    "Ix = jnp.ones((*batch_shape, n_col))\n",
    "Ii = Ij = jnp.arange(n_col)\n",
    "mask = Cj[None,:] == Si[:, None]\n",
    "CSi = jnp.broadcast_to(Ci[None, :], mask.shape)[mask]\n",
    "CSi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2657582",
   "metadata": {},
   "source": [
    "> `CSi`: possible jittable alternative? how do we remove the zeros?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4ea2cb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "CSi_ = jnp.where(Cj[None, :] == Si[:, None], Ci[None, :], 0).sum(1) # not used\n",
    "CSi_ # not used"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44fc620e",
   "metadata": {},
   "source": [
    "The column index `CSj` of $C_{int}S_{bd}$ can more easily be obtained:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f71fdd7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = (Cj[:, None] == Si[None, :]).any(0)\n",
    "CSj = Sj[mask]\n",
    "CSj"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99bf7684",
   "metadata": {},
   "source": [
    "> `CSj`: possible jittable alternative? how do we remove the zeros?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffa7bc82",
   "metadata": {},
   "outputs": [],
   "source": [
    "CSj_ = jnp.where(mask, Sj, 0) # not used\n",
    "CSj_ # not used"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0fc6d02",
   "metadata": {},
   "source": [
    "Finally, the values `CSx` of $C_{int}S_{bd}$ can be obtained as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59d2e1b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "if Sx.ndim > 1:\n",
    "    CSx = Sx[..., mask] # normally this should be enough\n",
    "else:\n",
    "    CSx = Sx[mask] # need separate case bc bug in JAX... see https://github.com/google/jax/issues/9050\n",
    "   \n",
    "CSx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c02035bd",
   "metadata": {},
   "source": [
    "> `CSx`: possible jittable alternative? how do we remove the zeros?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "107e9e80",
   "metadata": {},
   "outputs": [],
   "source": [
    "CSx_ = jnp.where(mask, Sx, 0.0) # not used\n",
    "CSx_ # not used"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9411edf5",
   "metadata": {},
   "source": [
    "Now we calculate $\\mathbb{1} - C_{int}S_{bd}$ in an *uncoalesced* way (we might have duplicate indices on the diagonal):\n",
    "\n",
    "> **uncoalesced**: having duplicate index combinations (i, j) in the representation possibly with different corresponding values. This is usually not a problem as in linear operations these values will end up to be summed, usually the behavior you want:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53c3f4ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "I_CSi = jnp.concatenate([CSi, Ii], -1)\n",
    "I_CSj = jnp.concatenate([CSj, Ij], -1)\n",
    "I_CSx = jnp.concatenate([-CSx, Ix], -1)\n",
    "print(I_CSi)\n",
    "print(I_CSj)\n",
    "print(I_CSx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "188e4606",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_col, n_rhs = Cext.shape\n",
    "print(n_col, n_rhs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e018c515",
   "metadata": {},
   "source": [
    "The batch shape dimension can generally speaking be anything (in the example here 0D). We need to do the necessary reshapings to make the batch shape 1D:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dad259e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_lhs = jnp.prod(jnp.array(batch_shape, dtype=jnp.int32))\n",
    "print(n_lhs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cf69da7",
   "metadata": {},
   "outputs": [],
   "source": [
    "Sx = Sx.reshape(n_lhs, -1)\n",
    "Sx.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "788294fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "I_CSx = I_CSx.reshape(n_lhs, -1)\n",
    "I_CSx.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec741031",
   "metadata": {},
   "source": [
    "We're finally ready to do the most important part of the calculation, which we conveniently leave to `klujax` and `SuiteSparse`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c1d8644",
   "metadata": {},
   "outputs": [],
   "source": [
    "inv_I_CS_Cext = solve_klu(I_CSi, I_CSj, I_CSx, Cext)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e51487df",
   "metadata": {},
   "source": [
    "one more sparse multiplication:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cccda072",
   "metadata": {},
   "outputs": [],
   "source": [
    "S_inv_I_CS_Cext = mul_coo(Si, Sj, Sx, inv_I_CS_Cext)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "627a98f7",
   "metadata": {},
   "source": [
    "And one more $C_{ext}$ multiplication which we do by clever indexing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12ff877c",
   "metadata": {},
   "outputs": [],
   "source": [
    "CextT_S_inv_I_CS_Cext = S_inv_I_CS_Cext[..., Cexti, :][..., :, Cextj]\n",
    "CextT_S_inv_I_CS_Cext"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e1c75b2",
   "metadata": {},
   "source": [
    "That's it! We found the S-matrix of the circuit. We just need to reshape the batch dimension back into the matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbf962af",
   "metadata": {},
   "outputs": [],
   "source": [
    "_, n, _ = CextT_S_inv_I_CS_Cext.shape\n",
    "S = CextT_S_inv_I_CS_Cext.reshape(*batch_shape, n, n)\n",
    "S"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f6dea4c",
   "metadata": {},
   "source": [
    "Oh and to complete the `SDense` representation we need to specify the port map as well:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21098a45",
   "metadata": {},
   "outputs": [],
   "source": [
    "port_map"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cfee3cf",
   "metadata": {},
   "source": [
    "## Algorithm Improvements"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26b70270",
   "metadata": {},
   "source": [
    "This algorithm is \n",
    "\n",
    "* very fast for large circuits 🙂\n",
    "\n",
    "This algorithm is however:\n",
    "\n",
    "* **not** jittable 😥\n",
    "* **not** differentiable 😥\n",
    "* **not** GPU-compatible 🙂\n",
    "\n",
    "There are probably still plenty of improvements possible for this algorithm:\n",
    "\n",
    "* **¿** make it jittable **?**\n",
    "* **¿** make it differentiable (requires making klujax differentiable first) **?**\n",
    "* **¿** make it GPU compatible (requires making suitesparse GPU compatible... probably not gonna happen)**?**\n",
    "\n",
    "Bottom line is... Do you know how to improve this algorithm or how to implement the above suggestions? Please open a Merge Request!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sax",
   "language": "python",
   "name": "sax"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
