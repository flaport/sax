{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b11a4aa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp nn.core"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad9dbb7f",
   "metadata": {},
   "source": [
    "# NN - Core\n",
    "\n",
    "> Core for SAX neural networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3512bc49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hide\n",
    "import matplotlib.pyplot as plt\n",
    "from fastcore.test import test_eq\n",
    "from pytest import approx, raises\n",
    "\n",
    "import os, sys; sys.stderr = open(os.devnull, \"w\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05f67381",
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "from __future__ import annotations\n",
    "\n",
    "from typing import Callable, Dict, Optional, Tuple, Union\n",
    "\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from sax.nn.utils import denormalize, normalize\n",
    "from sax.typing_ import Array, ComplexFloat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5409f7bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def preprocess(*params: ComplexFloat) -> ComplexFloat:\n",
    "    \"\"\"preprocess parameters\n",
    "\n",
    "    > Note: (1) all arguments are first casted into the same shape. (2) then pairs \n",
    "      of arguments are divided into each other to create relative arguments. (3) all \n",
    "      arguments are then stacked into one big tensor\n",
    "    \"\"\"\n",
    "    x = jnp.stack(jnp.broadcast_arrays(*params), -1)\n",
    "    assert isinstance(x, jnp.ndarray)\n",
    "    to_concatenate = [x]\n",
    "    for i in range(1, x.shape[-1]):\n",
    "        _x = jnp.roll(x, shift=i, axis=-1)\n",
    "        to_concatenate.append(x / _x)\n",
    "        to_concatenate.append(_x / x)\n",
    "    x = jnp.concatenate(to_concatenate, -1)\n",
    "    assert isinstance(x, jnp.ndarray)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e11602bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def dense(\n",
    "    weights: Dict[str, Array],\n",
    "    *params: ComplexFloat,\n",
    "    x_norm: Tuple[float, float] = (0.0, 1.0),\n",
    "    y_norm: Tuple[float, float] = (0.0, 1.0),\n",
    "    preprocess: Callable = preprocess,\n",
    "    activation: Callable = jax.nn.leaky_relu,\n",
    ") -> ComplexFloat:\n",
    "    \"\"\"simple dense neural network\"\"\"\n",
    "    x_mean, x_std = x_norm\n",
    "    y_mean, y_std = y_norm\n",
    "    x = preprocess(*params)\n",
    "    x = normalize(x, mean=x_mean, std=x_std)\n",
    "    for i in range(len([w for w in weights if w.startswith(\"w\")])):\n",
    "        x = activation(x @ weights[f\"w{i}\"] + weights.get(f\"b{i}\", 0.0))\n",
    "    y = denormalize(x, mean=y_mean, std=y_std)\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f22a32c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def generate_dense_weights(\n",
    "    key: Union[int, Array],\n",
    "    sizes: Tuple[int, ...],\n",
    "    input_names: Optional[Tuple[str, ...]] = None,\n",
    "    output_names: Optional[Tuple[str, ...]] = None,\n",
    "    preprocess=preprocess,\n",
    ") -> Dict[str, ComplexFloat]:\n",
    "    \"\"\"Generate the weights for a dense neural network\"\"\"\n",
    "\n",
    "    if isinstance(key, int):\n",
    "        key = jax.random.PRNGKey(key)\n",
    "    assert isinstance(key, jnp.ndarray)\n",
    "\n",
    "    sizes = tuple(s for s in sizes)\n",
    "    if input_names:\n",
    "        arr = preprocess(*jnp.ones(len(input_names)))\n",
    "        assert isinstance(arr, jnp.ndarray)\n",
    "        sizes = (arr.shape[-1],) + sizes\n",
    "    if output_names:\n",
    "        sizes = sizes + (len(output_names),)\n",
    "\n",
    "    keys = jax.random.split(key, 2 * len(sizes))\n",
    "    rand = jax.nn.initializers.lecun_normal()\n",
    "    weights = {}\n",
    "    for i, (m, n) in enumerate(zip(sizes[:-1], sizes[1:])):\n",
    "        weights[f\"w{i}\"] = rand(keys[2 * i], (m, n))\n",
    "        weights[f\"b{i}\"] = rand(keys[2 * i + 1], (1, n)).ravel()\n",
    "\n",
    "    return weights"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sax",
   "language": "python",
   "name": "sax"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
